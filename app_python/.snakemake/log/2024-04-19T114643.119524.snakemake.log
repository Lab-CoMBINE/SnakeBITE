Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                count
---------------  -------
snv_deepvariant        1
total                  1

Select jobs to execute...
Execute 1 jobs...

[Fri Apr 19 11:46:43 2024]
localrule snv_deepvariant:
    input: /data2/utility/references/genomics/hs37d5.decoy.fa, /data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam
    output: /data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant
    jobid: 0
    reason: Missing output files: /data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant
    wildcards: sample=canone
    resources: tmpdir=/tmp


        docker run         -v "$/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam":"$/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -v "$/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant":"$/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -v "$/data2/utility/references/genomics/hs37d5.decoy.fa":"$/data2/utility/references/genomics/hs37d5.decoy.fa"         kishwars/pepper_deepvariant:r0.8         run_pepper_margin_deepvariant call_variant         -b "$/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -f "$/data2/utility/references/genomics/hs37d5.decoy.fa"         -o "$/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -t "$1"         $ $ $ $ $ $ $ $ $ $ $         --ont_r10_q20
        
[Fri Apr 19 11:46:46 2024]
Error in rule snv_deepvariant:
    jobid: 0
    input: /data2/utility/references/genomics/hs37d5.decoy.fa, /data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam
    output: /data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant
    shell:
        
        docker run         -v "$/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam":"$/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -v "$/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant":"$/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -v "$/data2/utility/references/genomics/hs37d5.decoy.fa":"$/data2/utility/references/genomics/hs37d5.decoy.fa"         kishwars/pepper_deepvariant:r0.8         run_pepper_margin_deepvariant call_variant         -b "$/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -f "$/data2/utility/references/genomics/hs37d5.decoy.fa"         -o "$/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -t "$1"         $ $ $ $ $ $ $ $ $ $ $         --ont_r10_q20
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-04-19T114643.119524.snakemake.log
WorkflowError:
At least one job did not complete successfully.
