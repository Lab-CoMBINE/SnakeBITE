Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                count
---------------  -------
snv_deepvariant        1
total                  1

Select jobs to execute...
Execute 1 jobs...

[Fri Apr 19 11:47:46 2024]
localrule snv_deepvariant:
    input: /data2/utility/references/genomics/hs37d5.decoy.fa, /data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam
    output: /data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant
    jobid: 0
    reason: Missing output files: /data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant
    wildcards: sample=canone
    resources: tmpdir=/tmp


        docker run         -v "/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam":"/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -v "/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant":"/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -v "/data2/utility/references/genomics/hs37d5.decoy.fa":"/data2/utility/references/genomics/hs37d5.decoy.fa"         kishwars/pepper_deepvariant:r0.8         run_pepper_margin_deepvariant call_variant         -b "/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -f "/data2/utility/references/genomics/hs37d5.decoy.fa"         -o "/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -t "1"                            --ont_r10_q20
        
[Fri Apr 19 11:48:11 2024]
Error in rule snv_deepvariant:
    jobid: 0
    input: /data2/utility/references/genomics/hs37d5.decoy.fa, /data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam
    output: /data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant
    shell:
        
        docker run         -v "/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam":"/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -v "/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant":"/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -v "/data2/utility/references/genomics/hs37d5.decoy.fa":"/data2/utility/references/genomics/hs37d5.decoy.fa"         kishwars/pepper_deepvariant:r0.8         run_pepper_margin_deepvariant call_variant         -b "/data2/mbaragli/python_app/app_python/work_dir/canone/sorted_reads/canone_sorted.bam"         -f "/data2/utility/references/genomics/hs37d5.decoy.fa"         -o "/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant"         -t "1"                            --ont_r10_q20
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job snv_deepvariant since they might be corrupted:
/data2/mbaragli/python_app/app_python/work_dir/canone/snv/canone_deepvariant
Traceback (most recent call last):

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/cli.py", line 2068, in args_to_api
    dag_api.execute_workflow(

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/api.py", line 589, in execute_workflow
    workflow.execute(

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/workflow.py", line 1247, in execute
    raise e

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/workflow.py", line 1243, in execute
    success = self.scheduler.schedule()
              ^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/scheduler.py", line 199, in schedule
    self._error_jobs()

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/scheduler.py", line 388, in _error_jobs
    self._handle_error(job)

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/scheduler.py", line 440, in _handle_error
    async_run(

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/common/__init__.py", line 94, in async_run
    return asyncio.run(coroutine)
           ^^^^^^^^^^^^^^^^^^^^^^

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/jobs.py", line 1126, in postprocess
    await self.cleanup()

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/jobs.py", line 889, in cleanup
    await f.remove()

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/io.py", line 672, in remove
    await remove(self, remove_non_empty_dir=True, only_local=only_local)

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/io.py", line 958, in remove
    shutil.rmtree(file)

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/shutil.py", line 785, in rmtree
    _rmtree_safe_fd(fd, path, onexc)

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/shutil.py", line 686, in _rmtree_safe_fd
    _rmtree_safe_fd(dirfd, fullname, onexc)

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/shutil.py", line 717, in _rmtree_safe_fd
    onexc(os.unlink, fullname, err)

  File "/data2/abimbocci/.conda/envs/snakemake/lib/python3.12/shutil.py", line 715, in _rmtree_safe_fd
    os.unlink(entry.name, dir_fd=topfd)

PermissionError: [Errno 13] Permission denied: '1_pepper.log'

