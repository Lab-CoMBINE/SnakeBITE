Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job         count
--------  -------
minimap2        1
total           1

Select jobs to execute...
Execute 1 jobs...

[Tue Apr 23 09:45:32 2024]
localrule minimap2:
    input: /data2/utility/references/genomics/hs37d5.decoy.fa, /data2/abimbocci/snakemake_tutorial/data/samples/B.fastq
    output: /data2/mbaragli/python_app/app_python/work_dir/nohuppo3/mapped_reads/nohuppo3.bam
    jobid: 0
    reason: Missing output files: /data2/mbaragli/python_app/app_python/work_dir/nohuppo3/mapped_reads/nohuppo3.bam
    wildcards: sample=nohuppo3
    resources: tmpdir=/tmp

[Tue Apr 23 09:47:01 2024]
Error in rule minimap2:
    jobid: 0
    input: /data2/utility/references/genomics/hs37d5.decoy.fa, /data2/abimbocci/snakemake_tutorial/data/samples/B.fastq
    output: /data2/mbaragli/python_app/app_python/work_dir/nohuppo3/mapped_reads/nohuppo3.bam

Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-04-23T094532.607090.snakemake.log
WorkflowError:
At least one job did not complete successfully.
